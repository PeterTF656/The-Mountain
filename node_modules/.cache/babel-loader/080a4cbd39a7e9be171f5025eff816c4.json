{"ast":null,"code":"var _jsxFileName = \"/Users/zelinpu/Dev/FacialLandmarkDetection/src/EyeOne.js\";\nimport React, { useRef, useEffect, useState } from \"react\";\nimport { drawComponents } from \"./faceComponents\";\nimport * as facemesh from \"@tensorflow-models/face-landmarks-detection\";\n\nfunction EyeOne() {\n  const webcamRef = useRef(null);\n  const canvasRef = useRef(null);\n  const [draw, setDraw] = useState(0);\n  const [showCanvas, setShowCanvas] = useState(true);\n\n  const runFacemesh = async key => {\n    const net = await facemesh.load(facemesh.SupportedPackages.mediapipeFacemesh);\n    setInterval(() => {\n      detect(net, key);\n    }, 5);\n  };\n\n  const detect = async (net, key) => {\n    if (typeof webcamRef.current !== \"undefined\" && webcamRef.current !== null && webcamRef.current.video.readyState === 4) {\n      // Get Video Properties\n      const video = webcamRef.current.video;\n      const videoWidth = webcamRef.current.video.videoWidth;\n      const videoHeight = webcamRef.current.video.videoHeight; // Set video width\n\n      webcamRef.current.video.width = videoWidth;\n      webcamRef.current.video.height = videoHeight; // Set canvas width\n\n      if (canvasRef.current !== null) {\n        canvasRef.current.width = videoWidth;\n        canvasRef.current.height = videoHeight;\n      }\n\n      console.log(canvasRef); // Make Detections\n      // OLD MODEL\n      //       const face = await net.estimateFaces(video);\n      // NEW MODEL\n\n      const face = await net.estimateFaces({\n        input: video\n      }); // console.log(face);\n      // Get canvas context\n\n      if (canvasRef.current !== null) {\n        const ctx = canvasRef.current.getContext(\"2d\");\n        requestAnimationFrame(() => {\n          drawComponents(face, ctx, 1);\n        });\n      }\n    }\n  };\n\n  useEffect(() => {\n    runFacemesh();\n    return () => {};\n  }, [showCanvas]);\n  return /*#__PURE__*/React.createElement(\"div\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 59,\n      columnNumber: 9\n    }\n  }, showCanvas === true && /*#__PURE__*/React.createElement(\"canvas\", {\n    ref: canvasRef,\n    style: {\n      position: \"absolute\",\n      marginLeft: \"auto\",\n      marginRight: \"auto\",\n      left: 0,\n      right: 0,\n      textAlign: \"center\",\n      zindex: 9,\n      width: 1000,\n      height: 750\n    },\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 61,\n      columnNumber: 9\n    }\n  }));\n}\n\nexport default EyeOne;","map":{"version":3,"sources":["/Users/zelinpu/Dev/FacialLandmarkDetection/src/EyeOne.js"],"names":["React","useRef","useEffect","useState","drawComponents","facemesh","EyeOne","webcamRef","canvasRef","draw","setDraw","showCanvas","setShowCanvas","runFacemesh","key","net","load","SupportedPackages","mediapipeFacemesh","setInterval","detect","current","video","readyState","videoWidth","videoHeight","width","height","console","log","face","estimateFaces","input","ctx","getContext","requestAnimationFrame","position","marginLeft","marginRight","left","right","textAlign","zindex"],"mappings":";AAAA,OAAOA,KAAP,IAAgBC,MAAhB,EAAwBC,SAAxB,EAAmCC,QAAnC,QAAoD,OAApD;AACA,SAAQC,cAAR,QAA6B,kBAA7B;AACA,OAAO,KAAKC,QAAZ,MAA0B,6CAA1B;;AAIA,SAASC,MAAT,GAAkB;AAChB,QAAMC,SAAS,GAAGN,MAAM,CAAC,IAAD,CAAxB;AACA,QAAMO,SAAS,GAAGP,MAAM,CAAC,IAAD,CAAxB;AACA,QAAM,CAACQ,IAAD,EAAOC,OAAP,IAAkBP,QAAQ,CAAC,CAAD,CAAhC;AACA,QAAM,CAACQ,UAAD,EAAaC,aAAb,IAA8BT,QAAQ,CAAC,IAAD,CAA5C;;AAEF,QAAMU,WAAW,GAAG,MAAOC,GAAP,IAAe;AAC/B,UAAMC,GAAG,GAAG,MAAMV,QAAQ,CAACW,IAAT,CAAcX,QAAQ,CAACY,iBAAT,CAA2BC,iBAAzC,CAAlB;AACAC,IAAAA,WAAW,CAAC,MAAM;AAChBC,MAAAA,MAAM,CAACL,GAAD,EAAMD,GAAN,CAAN;AACD,KAFU,EAER,CAFQ,CAAX;AAGD,GALH;;AAOE,QAAMM,MAAM,GAAG,OAAOL,GAAP,EAAYD,GAAZ,KAAoB;AACjC,QACE,OAAOP,SAAS,CAACc,OAAjB,KAA6B,WAA7B,IACAd,SAAS,CAACc,OAAV,KAAsB,IADtB,IAEAd,SAAS,CAACc,OAAV,CAAkBC,KAAlB,CAAwBC,UAAxB,KAAuC,CAHzC,EAIE;AACA;AACA,YAAMD,KAAK,GAAGf,SAAS,CAACc,OAAV,CAAkBC,KAAhC;AACA,YAAME,UAAU,GAAGjB,SAAS,CAACc,OAAV,CAAkBC,KAAlB,CAAwBE,UAA3C;AACA,YAAMC,WAAW,GAAGlB,SAAS,CAACc,OAAV,CAAkBC,KAAlB,CAAwBG,WAA5C,CAJA,CAMA;;AACAlB,MAAAA,SAAS,CAACc,OAAV,CAAkBC,KAAlB,CAAwBI,KAAxB,GAAgCF,UAAhC;AACAjB,MAAAA,SAAS,CAACc,OAAV,CAAkBC,KAAlB,CAAwBK,MAAxB,GAAiCF,WAAjC,CARA,CAUA;;AACA,UAAIjB,SAAS,CAACa,OAAV,KAAsB,IAA1B,EAAgC;AAChCb,QAAAA,SAAS,CAACa,OAAV,CAAkBK,KAAlB,GAA0BF,UAA1B;AACAhB,QAAAA,SAAS,CAACa,OAAV,CAAkBM,MAAlB,GAA2BF,WAA3B;AACC;;AACDG,MAAAA,OAAO,CAACC,GAAR,CAAYrB,SAAZ,EAfA,CAiBA;AACA;AACA;AACA;;AACA,YAAMsB,IAAI,GAAG,MAAMf,GAAG,CAACgB,aAAJ,CAAkB;AAACC,QAAAA,KAAK,EAACV;AAAP,OAAlB,CAAnB,CArBA,CAsBA;AACA;;AACA,UAAId,SAAS,CAACa,OAAV,KAAsB,IAA1B,EAAgC;AAChC,cAAMY,GAAG,GAAGzB,SAAS,CAACa,OAAV,CAAkBa,UAAlB,CAA6B,IAA7B,CAAZ;AACAC,QAAAA,qBAAqB,CAAC,MAAI;AAAC/B,UAAAA,cAAc,CAAC0B,IAAD,EAAOG,GAAP,EAAY,CAAZ,CAAd;AAA6B,SAAnC,CAArB;AACC;AACF;AACF,GAlCD;;AAoCA/B,EAAAA,SAAS,CAAC,MAAI;AAACW,IAAAA,WAAW;AAAI,WAAO,MAAM,CAAE,CAAf;AAAgB,GAArC,EAAsC,CAACF,UAAD,CAAtC,CAAT;AAEA,sBACM;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,KACCA,UAAU,KAAK,IAAf,iBACD;AACE,IAAA,GAAG,EAAEH,SADP;AAEE,IAAA,KAAK,EAAE;AACL4B,MAAAA,QAAQ,EAAE,UADL;AAELC,MAAAA,UAAU,EAAE,MAFP;AAGLC,MAAAA,WAAW,EAAE,MAHR;AAILC,MAAAA,IAAI,EAAE,CAJD;AAKLC,MAAAA,KAAK,EAAE,CALF;AAMLC,MAAAA,SAAS,EAAE,QANN;AAOLC,MAAAA,MAAM,EAAE,CAPH;AAQLhB,MAAAA,KAAK,EAAE,IARF;AASLC,MAAAA,MAAM,EAAE;AATH,KAFT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAFA,CADN;AAmBD;;AAED,eAAerB,MAAf","sourcesContent":["import React, { useRef, useEffect, useState, } from \"react\";\nimport {drawComponents} from \"./faceComponents\";\nimport * as facemesh from \"@tensorflow-models/face-landmarks-detection\";\n\n\n\nfunction EyeOne() {\n  const webcamRef = useRef(null);\n  const canvasRef = useRef(null);\n  const [draw, setDraw] = useState(0);\n  const [showCanvas, setShowCanvas] = useState(true)\n\nconst runFacemesh = async (key) => {\n    const net = await facemesh.load(facemesh.SupportedPackages.mediapipeFacemesh);\n    setInterval(() => {\n      detect(net, key);\n    }, 5);\n  };\n\n  const detect = async (net, key) => {\n    if (\n      typeof webcamRef.current !== \"undefined\" &&\n      webcamRef.current !== null &&\n      webcamRef.current.video.readyState === 4\n    ) {\n      // Get Video Properties\n      const video = webcamRef.current.video;\n      const videoWidth = webcamRef.current.video.videoWidth;\n      const videoHeight = webcamRef.current.video.videoHeight;\n\n      // Set video width\n      webcamRef.current.video.width = videoWidth;\n      webcamRef.current.video.height = videoHeight;\n\n      // Set canvas width\n      if (canvasRef.current !== null) {\n      canvasRef.current.width = videoWidth;\n      canvasRef.current.height = videoHeight;\n      }\n      console.log(canvasRef);\n\n      // Make Detections\n      // OLD MODEL\n      //       const face = await net.estimateFaces(video);\n      // NEW MODEL\n      const face = await net.estimateFaces({input:video});\n      // console.log(face);\n      // Get canvas context\n      if (canvasRef.current !== null) {\n      const ctx = canvasRef.current.getContext(\"2d\");\n      requestAnimationFrame(()=>{drawComponents(face, ctx, 1)});\n      }\n    }\n  };\n\n  useEffect(()=>{runFacemesh(); return () => {}},[showCanvas]);\n\n  return (\n        <div>\n        {showCanvas === true &&\n        <canvas\n          ref={canvasRef}\n          style={{\n            position: \"absolute\",\n            marginLeft: \"auto\",\n            marginRight: \"auto\",\n            left: 0,\n            right: 0,\n            textAlign: \"center\",\n            zindex: 9,\n            width: 1000,\n            height: 750,\n          }}\n        />}\n    </div>\n  );\n}\n\nexport default EyeOne;\n"]},"metadata":{},"sourceType":"module"}